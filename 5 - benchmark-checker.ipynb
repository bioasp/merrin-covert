{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark - Error calculation -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "from colomoto import minibn\n",
    "from merrin.bnutils import bn_score_influence_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = 'Growth'\n",
    "regulators = ['RPcl', 'RPO2', 'RPh', 'RPb']\n",
    "inputs = ['Carbon1', 'Carbon2', 'Oxygen', 'Fext', 'Hext']\n",
    "outputs = ['Biomass', 'Dext', 'Eext']\n",
    "\n",
    "# Files\n",
    "reference_simulations = 'data/covert/result/pipeline_nbi/FlexFlux/*.csv'\n",
    "\n",
    "# Solver statistics\n",
    "inferring_statistics = './benchmark/results/inferring_statistics.csv'\n",
    "\n",
    "# BNET\n",
    "bn_by_instance = './benchmark/results/bn_by_instances.csv'\n",
    "bnet_dir = './benchmark/results/bnet/BN-{}.bnet'\n",
    "\n",
    "# Simulations\n",
    "simulations_dir = './benchmark/results/simulations/*'\n",
    "\n",
    "# Export\n",
    "csv_export = './benchmark/results/{}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_sum_of_squares(X: pd.DataFrame, Y: pd.DataFrame, normalize:bool = False):\n",
    "    assert(X.size == Y.size)\n",
    "    assert(X.columns.tolist() == Y.columns.tolist())\n",
    "    \n",
    "    # normalization\n",
    "    if normalize:\n",
    "        X_mean = X.mean(axis=0)\n",
    "        X_std = X.std(axis=0)\n",
    "        X_ = (X - X_mean) / X_std\n",
    "        Y_ = (Y - X_mean) / X_std\n",
    "    else:\n",
    "        X_ = X\n",
    "        Y_ = Y\n",
    "\n",
    "    # rss\n",
    "    Z = (X_ - Y_)**2\n",
    "    rss = Z.sum(axis=1).sum()\n",
    "    return rss\n",
    "\n",
    "def mean_squared_error(X, Y):\n",
    "    assert(X.size == Y.size)\n",
    "    assert(X.columns.tolist() == Y.columns.tolist())\n",
    "    rss = residual_sum_of_squares(X, Y)\n",
    "    mss = rss / X.size\n",
    "    return mss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(ref_df: dict, sim_df: dict):\n",
    "    assert(sorted(ref_df.keys()) == sorted(sim_df.keys()))\n",
    "\n",
    "    errors = {'RSS_External_Sum': 0, 'RSS_Regulators_Sum': 0}\n",
    "    for k in ref_df.keys():\n",
    "        ref = ref_df[k]\n",
    "        sim = sim_df[k]\n",
    "\n",
    "        rss_inout = residual_sum_of_squares(ref[inputs + outputs], sim[inputs + outputs], normalize=True)\n",
    "        rss_reg = residual_sum_of_squares(ref[regulators], sim[regulators], normalize=False)\n",
    "\n",
    "        errors[f'RSS_External_{k}'] = rss_inout\n",
    "        errors[f'RSS_Regulators_{k}'] = rss_reg\n",
    "\n",
    "        errors['RSS_External_Sum'] += rss_inout\n",
    "        errors['RSS_Regulators_Sum'] += rss_reg\n",
    "\n",
    "    return errors\n",
    "\n",
    "def load_sim(sim_path):\n",
    "    sims = {}\n",
    "    for sim in glob(sim_path):\n",
    "        name = sim.split('/')[-1].strip('.csv')[-4:]\n",
    "        sims[name] = pd.read_csv(sim, sep='\\t')\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean networks analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean network error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sims_df = load_sim(reference_simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unreg_sims_df = load_sim('data/covert_no_reg/out/pipeline_nbi/FlexFlux/*.csv')\n",
    "worst_case_error_ext = compute_error(ref_sims_df, unreg_sims_df)['RSS_External_Sum']\n",
    "worst_case_error_reg = 5 * 301 * 4 # 5 simulations with 301 time steps with 4 regulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_case_error_ext, worst_case_error_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = {}\n",
    "\n",
    "for bn_folder in glob(simulations_dir):\n",
    "    ref_bn = int(bn_folder.strip('/').split('-')[-1])\n",
    "    sims_df = load_sim(bn_folder + '/*.csv')\n",
    "    sim_errors = compute_error(ref_sims_df, sims_df)\n",
    "    errors[ref_bn] = {'Ref_BN': int(ref_bn)} | sim_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(errors).T\n",
    "df = df[sorted(df.columns)]\n",
    "\n",
    "df = df.sort_values(['Ref_BN'])\n",
    "df['Ref_BN'] = df['Ref_BN'].astype(int)\n",
    "df = df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "gt_bnet = minibn.BooleanNetwork.load('data/covert/regulatory_network.bnet')\n",
    "for ref_bn in df.index:\n",
    "    bnet = minibn.BooleanNetwork.load(bnet_dir.format(ref_bn))\n",
    "    score = {'Ref_BN': ref_bn} | bn_score_influence_graph(gt_bnet, bnet)\n",
    "    scores.append(score)\n",
    "df = df.merge(pd.DataFrame(scores).rename(columns={'recall': 'BN_score_recall', 'precision': 'BN_score_precision'}), on='Ref_BN', how='left')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_external_col = [f'RSS_External_{s}' for s in ['fig5', 'fig6', 'fig7', 'fig8', 'fig9']]\n",
    "rss_reg_col = [f'RSS_Regulators_{s}' for s in ['fig5', 'fig6', 'fig7', 'fig8', 'fig9']]\n",
    "bin_df = df[rss_reg_col + rss_external_col]\n",
    "for s in ['fig5', 'fig6', 'fig7', 'fig8', 'fig9']:\n",
    "    bin_df[s] = (bin_df[f'RSS_External_{s}'] + bin_df[f'RSS_Regulators_{s}'])\n",
    "df['Nb_Perfect_match_simulations'] = bin_df[['fig5', 'fig6', 'fig7', 'fig8', 'fig9']].astype(bool).sum(axis=1).apply(lambda x: 5 - x)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Simulation_Correctness_External'] = df['RSS_External_Sum'] #1 - df['RSS External - Sum'] / worst_case_error_ext\n",
    "df['Simulation_Correctness_Regulators'] = df['RSS_Regulators_Sum'] / worst_case_error_reg\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(csv_export.format('bn_statistics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_bn = pd.read_csv(bn_by_instance)\n",
    "instance_bn = instance_bn.drop('Unnamed: 0', axis=1)\n",
    "# instance_bn = instance_bn.rename({'Data type': 'Data_type', 'Degradation %': 'Degradation', 'Solution ID': 'Solution_ID', 'Ref BN': 'Ref_BN'}, axis=1)\n",
    "instance_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferring_statistics_df = pd.read_csv(inferring_statistics)\n",
    "# inferring_statistics_df = inferring_statistics_df.rename({'Data type': 'Data_type', 'Degradation %': 'Degradation'}, axis=1)\n",
    "inferring_statistics_df[inferring_statistics_df['Status'] != 'SAT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = instance_bn.merge(df, on='Ref_BN', how='left')\n",
    "full_df = full_df.sort_values(['Instance', 'Data_type', 'Degradation', 'Seed', 'Solution_ID'])\n",
    "full_df = full_df.reset_index().drop('index', axis=1)\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv('./benchmark/results/all_data_statistics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Worst case results - by instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_columns = ['Instance', 'Data_type', 'Degradation', 'Seed']\n",
    "\n",
    "dg = full_df[level_columns].drop_duplicates().reset_index().drop('index', axis=1)\n",
    "\n",
    "dg['Nb_Solution'] = dg.join(full_df.groupby(level_columns)['Solution_ID'].count(), on=level_columns, rsuffix='_r')['Solution_ID']\n",
    "\n",
    "dg['Best_Score_External'] = dg.join(full_df.groupby(level_columns)['Simulation_Correctness_External'].min(), on=level_columns, rsuffix='_r')['Simulation_Correctness_External']\n",
    "dg['Worst_Score_External'] = dg.join(full_df.groupby(level_columns)['Simulation_Correctness_External'].max(), on=level_columns, rsuffix='_r')['Simulation_Correctness_External']\n",
    "\n",
    "dg['Best_Score_Regulators'] = dg.join(full_df.groupby(level_columns)['Simulation_Correctness_Regulators'].min(), on=level_columns, rsuffix='_r')['Simulation_Correctness_Regulators']\n",
    "dg['Worst_Score_Regulators'] = dg.join(full_df.groupby(level_columns)['Simulation_Correctness_Regulators'].max(), on=level_columns, rsuffix='_r')['Simulation_Correctness_Regulators']\n",
    "\n",
    "dg['Best_nb_Perfect_match_Simulation'] = dg.join(full_df.groupby(level_columns)['Nb_Perfect_match_simulations'].max(), on=level_columns, rsuffix='_r')['Nb_Perfect_match_simulations']\n",
    "dg['Worst_nb_Perfect_match_Simulation'] = dg.join(full_df.groupby(level_columns)['Nb_Perfect_match_simulations'].min(), on=level_columns, rsuffix='_r')['Nb_Perfect_match_simulations']\n",
    "\n",
    "dg['Best_BN_score_recall'] = dg.join(full_df.groupby(level_columns)['BN_score_recall'].max(), on=level_columns, rsuffix='_r')['BN_score_recall']\n",
    "dg['Worst_BN_score_recall'] = dg.join(full_df.groupby(level_columns)['BN_score_recall'].min(), on=level_columns, rsuffix='_r')['BN_score_recall']\n",
    "\n",
    "dg['Best_BN_score_precision'] = dg.join(full_df.groupby(level_columns)['BN_score_precision'].max(), on=level_columns, rsuffix='_r')['BN_score_precision']\n",
    "dg['Worst_BN_score_precision'] = dg.join(full_df.groupby(level_columns)['BN_score_precision'].min(), on=level_columns, rsuffix='_r')['BN_score_precision']\n",
    "\n",
    "# dg = dg.round(4)\n",
    "dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in inferring_statistics_df[inferring_statistics_df['Status'] != 'SAT'].iterrows():\n",
    "    instance = r['Instance']\n",
    "    datatype = r['Data_type']\n",
    "    degradation = r['Degradation']\n",
    "    seed = r['Seed']\n",
    "    assert(not ((dg['Instance'] == instance) & (dg['Data_type'] == datatype) & (dg['Degradation'] == degradation) & (dg['Seed'] == seed)).any())\n",
    "    added_row = {\n",
    "        'Instance': instance,\n",
    "        'Data_type': datatype,\n",
    "        'Degradation': degradation,\n",
    "        'Seed': seed\n",
    "    }\n",
    "    dg = dg.append(added_row, ignore_index = True)\n",
    "dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg.to_csv('./benchmark/results/instance_statistics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Worst case results - by degradation level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_columns = ['Instance', 'Data_type', 'Degradation']\n",
    "\n",
    "dh = dg[level_columns].drop_duplicates().reset_index().drop('index', axis=1)\n",
    "\n",
    "dh['Min_nb_Solution'] = dh.join(dg.groupby(level_columns)['Nb_Solution'].min(), on=level_columns, rsuffix='_r')['Nb_Solution']\n",
    "dh['Max_nb_Solution'] = dh.join(dg.groupby(level_columns)['Nb_Solution'].max(), on=level_columns, rsuffix='_r')['Nb_Solution']\n",
    "\n",
    "dh['Nb_Different_Solution'] = dh.join(full_df.groupby(level_columns)['Ref_BN'].nunique(), on=level_columns, rsuffix='_r')['Ref_BN']\n",
    "\n",
    "dh['Best_Score_External'] = dh.join(dg.groupby(level_columns)['Best_Score_External'].min(), on=level_columns, rsuffix='_r')['Best_Score_External']\n",
    "dh['Worst_Score_External'] = dh.join(dg.groupby(level_columns)['Worst_Score_External'].max(), on=level_columns, rsuffix='_r')['Worst_Score_External']\n",
    "\n",
    "dh['Best_Score_Regulators'] = dh.join(dg.groupby(level_columns)['Best_Score_Regulators'].min(), on=level_columns, rsuffix='_r')['Best_Score_Regulators']\n",
    "dh['Worst_Score_Regulators'] = dh.join(dg.groupby(level_columns)['Worst_Score_Regulators'].max(), on=level_columns, rsuffix='_r')['Worst_Score_Regulators']\n",
    "\n",
    "dh['Best_nb_Perfect_match_Simulation'] = dh.join(dg.groupby(level_columns)['Best_nb_Perfect_match_Simulation'].max(), on=level_columns, rsuffix='_r')['Best_nb_Perfect_match_Simulation']\n",
    "dh['Worst_nb_Perfect_match_Simulation'] = dh.join(dg.groupby(level_columns)['Worst_nb_Perfect_match_Simulation'].min(), on=level_columns, rsuffix='_r')['Worst_nb_Perfect_match_Simulation']\n",
    "\n",
    "dh['Best_BN_score_recall'] = dh.join(dg.groupby(level_columns)['Best_BN_score_recall'].max(), on=level_columns, rsuffix='_r')['Best_BN_score_recall']\n",
    "dh['Worst_BN_score_recall'] = dh.join(dg.groupby(level_columns)['Worst_BN_score_recall'].min(), on=level_columns, rsuffix='_r')['Worst_BN_score_recall']\n",
    "\n",
    "dh['Best_BN_score_precision'] = dh.join(dg.groupby(level_columns)['Best_BN_score_precision'].max(), on=level_columns, rsuffix='_r')['Best_BN_score_precision']\n",
    "dh['Worst_BN_score_precision'] = dh.join(dg.groupby(level_columns)['Worst_BN_score_precision'].min(), on=level_columns, rsuffix='_r')['Worst_BN_score_precision']\n",
    "\n",
    "dh['Nb_Unsat'] = dh.join(dg.groupby(level_columns)['Nb_Solution'].apply(lambda x : x.isna().sum()), on=level_columns, rsuffix='_r')['Nb_Solution']\n",
    "\n",
    "dh = dh.sort_values(level_columns).reset_index().drop('index', axis=1)\n",
    "\n",
    "# dh = dh.round(4)\n",
    "dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh.to_csv('./benchmark/results/global_statistics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
